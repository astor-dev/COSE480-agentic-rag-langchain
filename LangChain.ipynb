{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJBwQUGao4pu",
    "outputId": "aacd50be-6a85-4d9b-ede9-b79cd667ab34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: langchain-openai in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.61 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-openai) (0.3.61)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-openai) (1.82.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.11.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/do-huni/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xhV1dUaxo9Sd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CK7qPjsq_aN",
    "outputId": "9d2ecc90-f65e-4395-bf53-a495d87db4a7"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate,  HumanMessagePromptTemplate,  PipelinePromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FIgwVmGpNu5"
   },
   "source": [
    "# 기본 LLM\n",
    "\n",
    "\n",
    "\n",
    "*   ChatOpenAI : 모델을 불러오는 클래스\n",
    "*   ChatPromptTemplate : prompt 템플릿을 제공해주는 클래스\n",
    "*   ChatPromptTemplate.from_template() : 문자열 형태의 템플릿을 인자로 받아, 해당 형식에 맞는 프롬프트 객체를 생성\n",
    "*   StrOutputParaser : 모델을 출력을 문자열 형태로 파싱하여 최종 결과를 반환\n",
    "*   invoke : chain을 실행하는 메서드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOcMVbfOoyob",
    "outputId": "fa57b6b1-5efe-4cc5-8070-4941582e8e6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='지구의 자전 주기는 약 24시간입니다. 이것은 하루의 길이와 같은 시간을 의미합니다. 하루의 길이는 일출과 일몰이 발생하는 시간에 영향을 줍니다. 지구는 자전하는 동안 태양 주위를 공전하면서 자전하는 데 걸리는 시간은 약 24시간입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 16, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbLYIHykXSFR5LTkf8Viuhg3nh8IO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2bccd0db-17bf-46ea-a25c-ed1524494e54-0', usage_metadata={'input_tokens': 16, 'output_tokens': 112, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 간단한 LLM 실행\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "llm.invoke(\"지구의 자전 주기는?\")\n",
    "# llm.invoke(\"지구의 자전 주기는?\").content # 출력 내용만 가져옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdOtQT1pp6hy",
    "outputId": "42d3577b-e754-452e-d542-f1c43827efad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in astronomy. Answer the question. <Question>: {input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt template 도입\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X41QNZTqWRT",
    "outputId": "244050e6-e984-4007-a646-05821e9a8dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='지구의 자전 주기는 약 24시간, 즉 하루입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 30, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbHkdZFp5WmG8K3gBc6gXtCC4du0N', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--adae98db-ebb9-4264-9352-0638be3cbc40-0', usage_metadata={'input_tokens': 30, 'output_tokens': 23, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain으로 연결\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wRH1hbidqY-O",
    "outputId": "989bc72e-3e6d-4013-ac28-be4f5483e854"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'지구의 자전 주기는 약 24시간입니다. 이는 하루 동안 지구가 자전하는 시간을 나타내며, 우리가 낮과 밤을 경험하는 이유입니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STrOutputParser 도입.\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "-Rktc35Pqp2G",
    "outputId": "9fee8f05-c486-47a4-b1a2-41dc1fb3b88a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'According to the Oxford Dictionary, the word \\'future\\' is defined as \"the time or a period of time following the moment of speaking or writing; time regarded as still to come.\" This definition aligns with the concept of \\'미래\\' in Korean, as it also refers to a time that is yet to come.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 순차적인 체인 연결\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"explain {english_word} using oxford dictionary to me in Korean.\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"english_word\": chain1}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"korean_word\":\"미래\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrIKyFtXruLV"
   },
   "source": [
    "# 프롬프트 작성 규칙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx3TsX9YrEp8",
    "outputId": "22331140-96b7-4b4b-e1a4-d5ee7b6081e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 기업 실적 발표: 다음 주에는 여러 기업들이 1분기 실적을 발표할 예정이며, 이로 인해 해당 기업의 주가에 영향을 줄 수 있습니다.\n",
      "\n",
      "2. 미국 중간 선거: 미국의 중간 선거가 예정되어 있으며, 이는 미국 주식 시장에 영향을 줄 수 있습니다.\n",
      "\n",
      "3. 경제 지표 발표: 다음 주에는 여러 나라에서 경제 지표가 발표될 예정이며, 이는 시장의 건강 상태에 대한 징후로 작용할 수 있습니다.\n",
      "\n",
      "4. 세계 경제 상황: 전 세계의 경제 상황이 계속해서 변화하고 있으며, 다음 주에 예상치 못한 사건이 발생할 수 있습니다. 이는 글로벌 시장에 영향을 줄 수 있습니다.\n",
      "------------------------------\n",
      "주식 시장은 회사가 주식을 발행하여 자금을 조달하고, 이를 투자하는 투자자들이 거래하는 곳을 말합니다. 일반적으로는 증권사나 주식거래소를 통해 거래가 이루어지며, 투자자들은 회사의 경영상태나 성과에 기반해 주식을 매수 또는 매도하여 수익을 얻을 수 있습니다.\n",
      "\n",
      "주식 시장은 주식시장의 종류에 따라 국내시장과 해외시장으로 나눌 수 있습니다. 국내시장에서는 한국거래소(KRX)를 중심으로 코스피와 코스닥이 대표적인 주식시장이며, 해외시장에서는 미국의 뉴욕증시나 나스닥, 일본의 도쿄증시 등이 대표적인 주식시장입니다.\n",
      "\n",
      "주식 시장에서는 주식뿐만 아니라 채권, 파생상품 등 다양한 금융상품을 거래할 수 있으며, 주식 시장의 변동은 경제 상황이나 기업 실적, 정치적 요인 등 다양한 요인에 영향을 받습니다. 주식 시장은 투기와 투자의 장으로 평가되며, 적절한 정보 확보와 투자 전략을 세우는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# 명확성과 구체성\n",
    "question1 = \"다음 주 주식 시장에 영향을 줄 수 있는 예정된 이벤트들은 무엇일까요?\"\n",
    "question2 = \"주식 시장에 대해 알려주세요.\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwEYlDHrspCD"
   },
   "outputs": [],
   "source": [
    "# 배경 정보를 포함\n",
    "question1 = \"2020년 미국 대선의 결과를 바탕으로 현재 정치 상황에 대한 분석을 해주세요.\"\n",
    "question2 = \"현재 정치 상황을 분석을 해주세요.\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mz37geoBs5cb",
    "outputId": "a2b9fdbf-2a79-4c80-f8df-a55624a2d051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021년 삼성전자의 ESG 보고서에는 기업의 환경, 사회 및 지배구조에 대한 다양한 정보와 노력들이 담겨 있습니다. 삼성전자는 지속가능한 경영을 위해 다양한 사업 분야에서 ESG 관련 활동을 펼치고 있습니다.\n",
      "\n",
      "환경 측면에서는 기후변화 대응을 위해 온실가스 감축을 위한 목표를 설정하고 환경친화 제품 개발에 힘쓰고 있습니다. 또한 재활용을 촉진하고 환경 파괴를 최소화하기 위한 다양한 노력을 기울이고 있습니다.\n",
      "\n",
      "사회 측면에서는 다양성과 평등을 증진하기 위해 다양한 채용 프로그램과 교육 지원을 실시하고 있습니다. 또한 지역사회에 기여하기 위해 다양한 사회공헌 활동을 전개하고 있습니다.\n",
      "\n",
      "지배구조 측면에서는 투명하고 효과적인 기업 운영을 위해 총수회의체를 통해 의사결정을 내리고 이사회의 독립성을 강화하고 있습니다.\n",
      "\n",
      "이처럼 삼성전자는 ESG 관심사에 대해 다양한 노력을 기울이고 지속가능한 경영을 위해 노력하고 있습니다.\n",
      "------------------------------\n",
      "삼성전자의 2021년 ESG 보고서는 환경, 사회, 지배구조에 대한 다양한 정보를 담고 있습니다. 환경 부문에서는 기후변화 대응을 위한 친환경 제품 및 기술 개발, 탄소 중립 전략, 재생에너지 사용 등의 노력을 소개하고 있습니다. 사회 부문에서는 다양성과 포용을 증진하기 위한 다양한 프로그램과 사회공헌 활동, 고객 개인정보 보호 및 사이버 보안에 대한 노력 등을 다루고 있습니다. 지배구조 부문에서는 투명하고 효과적인 기업 운영을 위한 다양한 정책과 시스템을 소개하고 있습니다. 총회사의 ESG 실천과 성과를 종합적으로 평가하고 향후 방향을 제시하는데 있어 중요한 자료로 활용될 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 간결함.\n",
    "question1 = \"2021년에 발표된 삼성전자의 ESG 보고서를 요약해주세요.\"\n",
    "question2 = \"삼성전자의 다양한 보고서 중 특히 2021년에 발표된 ESG 보고서를 포함하여 전반적인 내용을 요약해주세요.\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkjEEzUStJQo",
    "outputId": "f512e8bc-c86a-448c-a78a-c41cf68bc876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신재생에너지 분야에서의 최신 연구 동향은 다음과 같습니다:\n",
      "\n",
      "1. 태양광 에너지: 태양전지의 효율을 높이기 위한 연구가 활발히 진행 중이며, 특히 페로브스카이트 태양전지와 페로브스카이트-신선형 태양전지 등의 기술이 주목받고 있습니다. 또한, 태양광 발전 소자의 경량화와 쉬운 설치를 위한 기술 개발이 진행 중에 있습니다.\n",
      "\n",
      "2. 풍력 에너지: 풍력터빈의 효율을 높이고 운전 및 유지보수 비용을 줄이기 위한 연구가 진행 중에 있습니다. 또한, 해상 풍력 발전을 위한 새로운 기술과 설비가 개발되고 있습니다.\n",
      "\n",
      "3. 지열 에너지: 친환경적이고 안정적인 지열 에너지를 활용하기 위한 지열 발전기술의 개선이 이루어지고 있습니다. 지열 열교환소 및 지열시축발전 시스템 등의 기술이 발전 중에 있습니다.\n",
      "\n",
      "4. 바이오매스 에너지: 바이오매스를 이용한 전기 생산 및 열 생산 기술의 향상이 이뤄지고 있습니다. 고체연료전지와 생물다양성을 보존하면서 바이오매스를 이용한 에너지생산 기술이 주목받고 있습니다.\n",
      "\n",
      "이러한 연구 결과를 토대로 신재생에너지 분야의 기술이 계속 발전하고 있으며, 미래에는 더욱 효율적이고 지속가능한 에너지 시스템이 구축될 것으로 기대됩니다.\n",
      "------------------------------\n",
      "신재생에너지는 지속가능하고 친환경적인 에너지원으로 인정받고 있습니다. 석탄, 석유와 같은 화석연료에 비해 대기오염, 기후변화 등에 미치는 영향이 적을 뿐만 아니라 고립된 지역에서도 발전이 가능하다는 장점이 있습니다. 또한, 재생에너지는 에너지 공급의 안정성을 높일 수 있으며, 지역 경제에 이바지할 수 있는 등 다양한 장점이 있습니다. 이러한 이유로 신재생에너지는 좋은 에너지원으로 인식되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 열린 질문 사용\n",
    "question1 = \"신재생에너지에 대한 최신 연구 동향은 무엇인가요?.\"\n",
    "question2 = \"신재생에너지가 좋은가요?\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmX7jc8Ltfg8",
    "outputId": "174d6583-4053-4ae0-9892-e787cdd6569c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 윤리에 대한 문제점은 다음과 같습니다. 먼저, AI 시스템이 인간의 편견과 선입견을 반영할 수 있어서 편향된 결정을 내릴 수 있다는 점이 문제점입니다. 또한 AI 시스템이 개인정보 보호, 개인의 자유와 권리를 침해할 수도 있습니다. 또한 AI가 인간의 일자리를 대체하고, 사람들의 신뢰를 훼손시킬 수도 있습니다.\n",
      "\n",
      "이러한 문제점을 해결하기 위한 방안으로는 먼저, 편향 데이터를 사용하지 않고 다양한 데이터를 활용하여 AI 시스템을 학습시키는 것이 중요합니다. 또한, AI 시스템을 설계할 때 윤리적인 가이드라인과 프레임워크를 고려하여 개발하는 것이 필요합니다. 또한, 인간의 관여와 감독이 중요하며, 투명성과 책임감 있는 운영이 필요합니다. 마지막으로, 개인정보 보호와 권리를 보호하기 위한 법률과 규정의 제고가 필요합니다. 이러한 방안을 통해 AI 윤리에 대한 문제를 해결할 수 있을 것으로 기대됩니다.\n",
      "------------------------------\n",
      "AI 윤리는 인공지능 기술이 사회적, 도덕적으로 적절하게 이용되어야 한다는 개념을 의미합니다. \n",
      "\n",
      "먼저, AI 윤리는 개인정보 보호와 권리를 존중해야 합니다. 개인정보를 수집, 저장, 처리할 때에는 사용자의 동의를 받아야 하고, 그 정보를 안전하게 보호해야 합니다. \n",
      "\n",
      "또한, AI 기술을 사용할 때에는 편향성과 차별을 방지해야 합니다. 데이터나 알고리즘 내에 내재된 편향을 인식하고 수정하여 공정하고 다양한 결정을 내릴 수 있도록 해야 합니다. \n",
      "\n",
      "또한, AI가 인간을 대신하여 결정을 내릴 때에는 책임과 투명성이 보장되어야 합니다. 결정이 내릴 때의 근거와 과정이 설명 가능해야 하고, 만일 문제가 발생했을 때 책임을 물을 수 있어야 합니다. \n",
      "\n",
      "AI 윤리는 인간 중심의 기술 개발을 지향합니다. 즉, 인공지능 기술이 인간을 위해 있는 것이라는 사고방식을 가지고, 사람들의 복지와 행복을 증진시키는 방향으로 사용되어야 한다는 점을 강조합니다. \n",
      "\n",
      "마지막으로, AI 윤리는 지속적인 논의와 업데이트가 필요합니다. 기술과 사회적 환경은 계속해서 변화하므로, 윤리적 이슈에 대한 새로운 관점과 해법을 찾아내는 과정이 중요합니다. \n",
      "\n",
      "AI 윤리는 기술 발전과 함께 발전해가야 하며, 인간과 기술이 함께 살아갈 수 있는 미래를 만들기 위해 계속해서 주의를 기울여야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 명확한 목표 설정\n",
    "question1 = \"AI 윤리에 대한 문제점과 해결 방안을 요약하여 설명해주세요.\"\n",
    "question2 = \"AI 윤리에 대해 이야기해주세요.\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDXj9RAxtv7b",
    "outputId": "1d30630d-70d2-4eff-ec4c-db752e1f8426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 경제에 대한 전망을 분석하기 전에 현재의 경제 상황을 살펴보겠습니다. 현재 미국은 코로나19 팬데믹으로 인해 경제적으로 어려움을 겪고 있습니다. 많은 기업들이 폐업하거나 일자리를 잃었고, 소비자 신뢰도가 낮아지는 등의 문제가 발생하고 있습니다.\n",
      "\n",
      "그러나 최근 경제 회복 조짐도 보이고 있습니다. 백신 접종률이 상승하면서 경제 활동이 원활해지고 소비도 증가하고 있습니다. 미국 정부의 경기부양책과 인프라투자 계획도 경제를 향상시키는 데 도움을 줄 것으로 예상됩니다.\n",
      "\n",
      "미국 경제의 전망은 여러 요소에 따라 달라질 수 있지만, 현재로서는 경제가 점차 회복되고 있는 모습을 보이고 있습니다. 그러나 여전히 미지수인 요인들도 존재하기 때문에 지켜봐야 할 부분이 많습니다.경제의 변화와 불확실성을 주시하면서 전문가들은 신중하게 경제의 전망을 분석하고 결정하여야 합니다.\n",
      "------------------------------\n",
      "미국은 세계 최대의 경제 규모를 가진 나라로, 고도의 산업화와 기술 혁신을 통해 번창하고 있습니다. 미국의 GDP는 세계 최대이며 다양한 산업 부문을 포괄하고 있습니다. 미국의 주요 산업 분야는 기술, 금융, 의료, 자동차, 에너지, 농업 등이 있는데, 특히 기술 분야에서 세계를 선도하고 있습니다.\n",
      "\n",
      "하지만 미국 경제는 최근 몇 년간 불안정한 요인에 직면해 왔습니다. 보유가 된 환율 문제와 거대한 국가부채, 그리고 중국과의 무역 분쟁 등이 미국 경제를 위협하고 있습니다. 특히, 코로나19 대유행은 미국 경제에도 큰 영향을 미쳤으며, 많은 기업의 폐업과 실업률 증가로 경제의 부정적인 영향을 크게 겪었습니다.\n",
      "\n",
      "미국은 경제적 자유와 경기 순환의 변동성이 큰 특징을 가지고 있어, 불안정한 경제 상황에 따라 미국의 경제 상황도 변화하고 있습니다. 현재 미국은 백신 접종률이 높아져 경제가 회복되는 모습을 보이고 있지만, 여전히 미지수가 많은 상황이기도 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 언어와 문체\n",
    "question1 = \"너는 미국 경제에 대해 분석하는 월스트리트 경제 전문가야. 미국 경제 전망에 대한 전문적인 분석을 부탁드립니다.\"\n",
    "question2 = \"미국 경제에 대해 말해봐\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain1 = llm | StrOutputParser()\n",
    "chain2 = llm | StrOutputParser()\n",
    "\n",
    "print(chain1.invoke(question1))\n",
    "print(\"-\" * 30)\n",
    "print(chain2.invoke(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36BhJPLgvIG2"
   },
   "source": [
    "# 프롬프트 Template\n",
    "\n",
    "* PromptTemplate : 프롬프트 template을 정의할 수 있는 Template\n",
    "* PromptTemplate.from_template : 문자열 template으로부터 PromptTemplate 인스턴스를 생성\n",
    "* PromptTemplate.format : 템플릿을 채우는 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7qfF4uhnuWiM",
    "outputId": "b1b6e221-e6ce-4c32-8770-616ab49aa91e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'안녕하세요, 제 이름은 홍길동이고, 나이는 30살입니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
    "\n",
    "filled_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c4A00XSuvo5Y",
    "outputId": "54cc0ce0-6c48-4091-ec44-9263832c86dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Hello, my name is Hong Gil-dong and I am 30 years old.\\n\\nI cannot call my father 'father.'\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 템플릿 결합\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
    "              + \"\\n\\n{language}로 번역해주세요.\"\n",
    ")\n",
    "\n",
    "combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"age\":30, \"language\":\"영어\", \"name\":\"홍길동\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq95Il80v1cn"
   },
   "source": [
    "# Chat Prompt Template\n",
    "\n",
    "* ChatPromptTemplate : 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용.\n",
    "* ChatPromptTemplate.from_messages : 메시지 리스트(혹은 튜플)을 기반으로 프롬프트를 구성함.\n",
    "* ChatPromptTemplate.format_messages : 사용자의 입력을 프롬프트에 동적으로 삽입하여, 최종적으로 대화형 상황을 반영한 메시지 리스트를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyejD-otvzMK",
    "outputId": "493f4000-4825-474f-8904-cbf1a72aed28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9dlocEdiyzz7",
    "outputId": "0f7a718b-d71a-4ede-aacd-76b1b1836b60"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'태양계에서 가장 큰 행성은 목성입니다. 목성은 지름이 약 14만 6천 킬로미터로 태양계의 다른 행성들보다 크기가 가장 큽니다.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "OOxyPyjby6va",
    "outputId": "087e0a84-cf1d-487a-af91-c9344f4c6bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}), HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'태양계에서 가장 큰 행성은 목성입니다. 목성은 질량과 부피 모두에서 태양계에서 가장 큰 행성이며, 지름은 약 143,000 km로 태양계에서 제일 큽니다.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuple 대신 MessagePromptTemplate을 사용할 수 있음.\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"), #role을 입력\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"), # content (메시지 내용) 을 입력\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "print(messages)\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "chain.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Rfpg9ybrzM5P",
    "outputId": "aa1dfc64-8a4d-4c9b-db15-a08f1d4816a6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'태양계에서 가장 큰 행성은 목성입니다. 목성은 가장 많은 질량과 부피를 가지고 있어 태양계에서 가장 큰 행성으로 알려져 있습니다.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q9ALzRbzWfl"
   },
   "source": [
    "# 그 외..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ubIettiJzVM4"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"\n",
    "                  이 질문에 추가 질문이 필요한가요: 예.\n",
    "                  추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "                  중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "                  추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "                  중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "                  최종 답변은: 아인슈타인\n",
    "                  \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"\n",
    "                  이 질문에 추가 질문이 필요한가요: 예.\n",
    "                  추가 질문: 네이버의 창립자는 누구인가요?\n",
    "                  중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "                  추가 질문: 이해진은 언제 태어났나요?\n",
    "                  중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "                  최종 답변은: 1967년 6월 22일\n",
    "                  \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"\n",
    "                  이 질문에 추가 질문이 필요한가요: 예.\n",
    "                  추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "                  중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "                  추가 질문: 신사임당은 언제 태어났나요?\n",
    "                  중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "                  추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "                  중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "                  최종 답변은: 연산군\n",
    "                  \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"\n",
    "                  이 질문에 추가 질문이 필요한가요: 예.\n",
    "                  추가 질문: 올드보이의 감독은 누구인가요?\n",
    "                  중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "                  추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "                  중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "                  추가 질문: 기생충의 감독은 누구인가요?\n",
    "                  중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "                  추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "                  중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "                  최종 답변은: 예\n",
    "                  \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVBhwBFYz-dg",
    "outputId": "1f6c2e10-3564-455a-8375-69bba19671fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n",
      "\n",
      "                  이 질문에 추가 질문이 필요한가요: 예.\n",
      "                  추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
      "                  중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
      "                  추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
      "                  중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
      "                  최종 답변은: 아인슈타인\n",
      "                  \n",
      "\n",
      "Question: 네이버의 창립자는 언제 태어났나요?\n",
      "\n",
      "                  이 질문에 추가 질문이 필요한가요: 예.\n",
      "                  추가 질문: 네이버의 창립자는 누구인가요?\n",
      "                  중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
      "                  추가 질문: 이해진은 언제 태어났나요?\n",
      "                  중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
      "                  최종 답변은: 1967년 6월 22일\n",
      "                  \n",
      "\n",
      "Question: 율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\n",
      "\n",
      "                  이 질문에 추가 질문이 필요한가요: 예.\n",
      "                  추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
      "                  중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
      "                  추가 질문: 신사임당은 언제 태어났나요?\n",
      "                  중간 답변: 신사임당은 1504년에 태어났습니다.\n",
      "                  추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
      "                  중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
      "                  최종 답변은: 연산군\n",
      "                  \n",
      "\n",
      "Question: 올드보이와 기생충의 감독이 같은 나라 출신인가요?\n",
      "\n",
      "                  이 질문에 추가 질문이 필요한가요: 예.\n",
      "                  추가 질문: 올드보이의 감독은 누구인가요?\n",
      "                  중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
      "                  추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
      "                  중간 답변: 박찬욱은 대한민국 출신입니다.\n",
      "                  추가 질문: 기생충의 감독은 누구인가요?\n",
      "                  중간 답변: 기생충의 감독은 봉준호입니다.\n",
      "                  추가 질문: 봉준호는 어느 나라 출신인가요?\n",
      "                  중간 답변: 봉준호는 대한민국 출신입니다.\n",
      "                  최종 답변은: 예\n",
      "                  \n",
      "\n",
      "Question: Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {question}\",\n",
    "    input_variables=[\"question\"], #프롬프트가 수신할 입력 항목\n",
    ")\n",
    "\n",
    "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
    "final_prompt = prompt.format(question=question)\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgOg8WAh0ev3",
    "outputId": "1db3ee30-bf73-4748-fe76-d1833a5d4d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 질문에 추가 질문이 필요한가요: 예.  \n",
      "추가 질문: Google은 언제 창립되었나요?  \n",
      "중간 답변: Google은 1998년 9월 4일에 창립되었습니다.  \n",
      "추가 질문: Bill Gates는 몇 년에 태어났나요?  \n",
      "중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.  \n",
      "추가 질문: 1998년 9월 4일 기준으로 Bill Gates의 나이는 몇 살인가요?  \n",
      "중간 답변: 1998년 9월 4일 기준 Bill Gates는 42세입니다.  \n",
      "최종 답변은: 42세\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(final_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMi8uM7L2y2Z"
   },
   "source": [
    "# 모델 파라미터 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lounenrq22KS"
   },
   "outputs": [],
   "source": [
    "# 모델 생성 단계에서 주기\n",
    "params = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,\n",
    "    \"presence_penalty\": 0.5,\n",
    "    \"stop\": [\"\\n\"]\n",
    "\n",
    "}\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **params, model_kwargs = kwargs)\n",
    "\n",
    "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "response = model.invoke(input=question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fy-rYt5m3LJt"
   },
   "outputs": [],
   "source": [
    "# 모델 호출 단계에서 주기\n",
    "params = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 10,\n",
    "}\n",
    "\n",
    "response = model.invoke(input=question, **params)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyWjXGNs3ZB3"
   },
   "outputs": [],
   "source": [
    "# bind 메서드를 통해 모델 인스턴스에 파라미터를 추가로 제공할 수 있음\n",
    "# 특수한 상황에서만 일부 파라미터를 다르게 적용할 수 있도록 해줌.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=100)\n",
    "\n",
    "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "\n",
    "before_answer = model.invoke(messages)\n",
    "\n",
    "print(before_answer)\n",
    "\n",
    "chain = prompt | model.bind(max_tokens=10)\n",
    "\n",
    "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
    "\n",
    "print(after_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMmfAxBg5VnM"
   },
   "source": [
    "# Parser\n",
    "\n",
    "*   CSV Parser\n",
    "  *  CommaSeparatedListOutputParser : 모델이 생성한 텍스트에서 쉼표로 구분된 항목을 추출\n",
    "  * get_format_instructions : 모델에 전달할 포멧 지시사항을 얻는 메서드\n",
    "*   Json Parser\n",
    "  * JsonOutputParser : 모델의 출력을 json으로 해석하며, Pydantic 모델에 맞게 데이터를 구조화하여 제공\n",
    "  * get_format_instructions : 모델에 전달할 포멧 지시사항을 얻는 메서드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYywD9Ue5b3o",
    "outputId": "13de3337-f7f1-452b-8238-730af55aeb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJtwDhfo6PMP",
    "outputId": "5e36246a-a2f1-4d5a-cded-50ef9480ab26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bibimbap', 'Kimchi', 'Bulgogi', 'Japchae', 'Tteokbokki']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\": \"popular Korean cusine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8gTRZ4U6csV",
    "outputId": "d14e6378-3d94-48a7-cdd3-9b9ebb566dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#자료구조 정의\n",
    "class CusineRecipe(BaseModel):\n",
    "    name: str = Field(description=\"name of a cusine\")\n",
    "    recipe: str = Field(description=\"recipe to cook the cusine\")\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1J1QHUsk69Xr",
    "outputId": "29ff8745-b541-4a85-acd1-25ec51ee2a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h45yRaHl7G3c",
    "outputId": "c7b6b6c6-2c59-4173-f741-a1ba1dc2d354"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bibimbap',\n",
       " 'recipe': 'Bibimbap is a Korean mixed rice dish that is served with an assortment of vegetables, meat, and a spicy sauce. To cook Bibimbap, start by preparing the rice according to package instructions. Then, sauté various vegetables such as spinach, carrots, bean sprouts, and mushrooms separately. Cook some thinly sliced beef in a hot pan with soy sauce and sugar. In a bowl, place the cooked rice at the bottom and arrange the vegetables and beef on top. Add a fried egg and gochujang (Korean chili paste) on the side. Mix everything together before eating. Enjoy your delicious Bibimbap!'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"query\": \"Let me know how to cook Bibimbap\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
